import { RagPiplineInput } from "../interfaces/interface";
import { RerankDocuments } from "../rerank/rerankDocuments";
import { ChatPromptTemplate, MessagesPlaceholder } from "@langchain/core/prompts";
import { RagNgineDocumentPromt } from "../prompts/prompt";
import { createStuffDocumentsChain } from "langchain/chains/combine_documents";
import { HumanMessage } from "@langchain/core/messages";

/**
 * A basic implementation of a Retrieval-Augmented Generation (RAG) pipeline.
 * This function processes a query, reranks the relevant documents, and generates a response using a language model.
 *
 * @param {RagPipelineInput} ragInput - The input configuration for the RAG pipeline.
 * @param {string} ragInput.query - The user's input query to retrieve relevant information.
 * @param {number} ragInput.topK - The number of top documents to retrieve.
 * @param {string} ragInput.rerank - The reranking method or strategy to apply to the retrieved documents.
 * @param {DocumentInterface[]} ragInput.docContent - An array of documents containing content to be used for retrieval.
 * @param {string} ragInput.ragPipeline - The identifier for the RAG pipeline configuration.
 * @param {EmbeddingsInstance} ragInput.embeddings - The instance of the embeddings model used for vectorizing documents.
 * @param {LLMInstance} ragInput.llm - The language model instance used for generating the response.
 *
 * @returns The response generated by the language model after processing the query and ranked documents.
 * 
 * @throws {Error} If any error occurs during document reranking or response generation.
 */
export default async function naiveRag(ragInput: RagPiplineInput) {
    try {
        const {embeddings, llm, docContent, rerank, topK, query} = ragInput;

        const rankedDocs = await RerankDocuments({query, topK, rerank}, docContent, embeddings);

        const prompt = ChatPromptTemplate.fromMessages([
            ["system", RagNgineDocumentPromt],
            new MessagesPlaceholder("messages"),
          ]);

        const docChain = await createStuffDocumentsChain({
            llm: llm,
            prompt
        });

        const response = await docChain.invoke({
            messages: [new HumanMessage(query)],
            context: rankedDocs,
          });

        return response

    } catch (error) {
        throw error;
    }
}